
# ğŸ“˜ Neural Confidence Journal Project (Weeks 1â€“10)
**Author:** Earnest Kyle  
**Course:** Deep Learning / NLP  
**Project:** Multi-Week Confidence Level Text Classification  

---

# â­ 1. Overview
This project analyzes **10 weeks of personal confidence journal entries**, training multiple NLP models to classify entries into:

- **0 = Low Confidence**
- **1 = Neutral Confidence**
- **2 = High Confidence**

The project progresses through increasingly advanced NLP approaches:

1. **Weeks 1â€“2** â€” EDA, TFâ€“IDF, PCA, baseline visualizations  
2. **Weeks 3â€“4** â€” Logistic Regression TFâ€“IDF classifier  
3. **Weeks 5â€“6** â€” Sentence embeddings + Feedforward Neural Network  
4. **Weeks 7â€“8** â€” DistilBERT fineâ€‘tuning  
5. **Weeks 9â€“10** â€” Retraining DistilBERT on expanded dataset + final evaluation  

---

# â­ 2. Folder Structure
```
NEURAL-CONFIDENCE-JOURNAL/
â”‚
â”œâ”€â”€ reports/
â”‚ â”œâ”€â”€ Final_Neural_Confidence_Journal_Report.pdf
â”‚ â”œâ”€â”€ Neural_Confidence_Journal_Weeks1-4_Report.pdf
â”‚ â”œâ”€â”€ Weeks_5-6_Progress_Report.pdf
â”‚ â”œâ”€â”€ Week_7_8_Report.pdf
â”‚ â”œâ”€â”€ Week9-10_Confidence_Report.pdf
â”‚ â””â”€â”€ (All progress reports delivered during the project)
â”‚
â”œâ”€â”€ week1-2_dataset/
â”‚ â”œâ”€â”€ confidence_journal_eda.ipynb
â”‚ â”œâ”€â”€ confidence_journal_week1-2.csv
â”‚ â”œâ”€â”€ README_week1-2.md
â”‚ â””â”€â”€ requirements_week1-2.txt
â”‚
â”œâ”€â”€ week3-4_baseline/
â”‚ â”œâ”€â”€ baseline_confidence_classifier_week3-4.ipynb
â”‚ â”œâ”€â”€ classification_report.txt
â”‚ â”œâ”€â”€ confusion_matrix_week3-4.png
â”‚ â”œâ”€â”€ baseline_lr_tfidf.joblib
â”‚ â”œâ”€â”€ best_model.joblib
â”‚ â””â”€â”€ README_week3-4.md
â”‚
â”œâ”€â”€ week5-6_embeddings/
â”‚ â”œâ”€â”€ week5-6_embeddings_and_nn.ipynb
â”‚ â”œâ”€â”€ ffnn_embeddings.keras
â”‚ â”œâ”€â”€ metrics.txt
â”‚ â”œâ”€â”€ preds_sample.csv
â”‚ â”œâ”€â”€ confusion_matrix_week5-6.png
â”‚ â”œâ”€â”€ requirements_week5-6.txt
â”‚ â””â”€â”€ README_week5-6.md
â”‚
â”œâ”€â”€ week7-8_distilbert/
â”‚ â”œâ”€â”€ week7-8_distilbert_finetune.ipynb
â”‚ â”œâ”€â”€ artifacts/
â”‚ â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ confusion_matrix_distilbert_week7-8.png
â”‚ â”œâ”€â”€ metrics.json
â”‚ â”œâ”€â”€ val_predictions_distilbert.csv
â”‚ â””â”€â”€ README_week7-8.md
â”‚
â”œâ”€â”€ week9-10_dataset/
â”‚ â”œâ”€â”€ confidence_journal_week9-10.csv
â”‚ â”œâ”€â”€ week9-10_confidence_analysis_and_retraining.ipynb
â”‚ â”œâ”€â”€ week9-10_confusion_matrix_using_old_model.png
â”‚ â”œâ”€â”€ week9-10_confusion_matrix_using_new_model.png
â”‚ â”œâ”€â”€ README_week9-10.md
â”‚ â””â”€â”€ requirements_week9-10.txt
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# â­ 3. Installation Instructions

## 3.1 Create Virtual Environment
```
python -m venv venv
source venv/bin/activate   # macOS / Linux
venv\Scripts\activate    # Windows
```

## 3.2 Install Dependencies
```
pip install -r requirements.txt
```

This installs all libraries needed for *every* week:
- PyTorch  
- TensorFlow  
- HuggingFace Transformers  
- Datasets  
- scikitâ€‘learn  
- Matplotlib / Seaborn  
- Joblib  
- Jupyter Notebook  

---

# â­ 4. Running Each Notebook

## âœ” Week 1â€“2: EDA & First Visualizations
File: `week1-2_eda/confidence_journal_eda.ipynb`  
Includes:
- Data inspection  
- TFâ€“IDF exploration  
- PCA scatter plot  
- Class distributions  
- Early confusion matrix  

---

## âœ” Week 3â€“4: Logistic Regression Baseline
File: `week3-4_baseline/baseline_confidence_classifier_week3-4.ipynb`  
Produces:
- TFâ€“IDF vectorization  
- Logistic Regression model  
- Saved models (`joblib`)  
- Classification report  
- Confusion matrix  

---

## âœ” Week 5â€“6: Embeddings + Neural Network
File: `week5-6_embeddings/week5-6_embeddings_and_nn.ipynb`  
Steps:
- Generate sentence embeddings with `sentence-transformers`  
- Train FFNN in TensorFlow/Keras  
- Generate predictions, metrics, confusion matrix  

---

## âœ” Week 7â€“8: DistilBERT Fine-Tuning
File: `week7-8_distilbert/week7-8_distilbert_finetune.ipynb`  
Includes:
- DistilBERT tokenizer + model  
- Full fineâ€‘tuning with HuggingFace Trainer  
- Metrics + confusion matrix  
- Saved model and tokenizer  

---

## âœ” Week 9â€“10: Retraining & Final Model
File: `week9-10_dataset/week9-10_confidence_analysis_and_retraining.ipynb`  
Features:
- Evaluation of old Week 7â€“8 model on Week 9â€“10 data  
- Retraining DistilBERT on combined data  
- Side-by-side comparison  
- New model confusion matrix (strong improvement)  
- Save final model for deployment  

---

# â­ 5. Results Summary

## ğŸ”¹ Week 1â€“2 Baseline Patterns
- PCA showed partially separable classes  
- Early predictions were weak but improved with TFâ€“IDF models  

## ğŸ”¹ Week 3â€“4 Logistic Regression Results
- Accuracy improved significantly (~70â€“75%)  
- Neutral class predicted most accurately  

## ğŸ”¹ Week 5â€“6 Embeddings + FFNN
- Learned richer text semantics  
- Improved F1 scores over TFâ€“IDF  

## ğŸ”¹ Week 7â€“8 DistilBERT Fineâ€‘Tuning
- Performance increased dramatically  
- Strong contextual understanding  

## ğŸ”¹ Week 9â€“10 Retrained Final Model
New DistilBERT model achieved:
- **Accuracy ~ 96%**
- **Precision/Recall/F1: 94â€“98%**
- Only **4 misclassified** entries out of 89  
- Large improvement over older Week 7â€“8 model (55% accuracy)

This makes it the definitive final model.

---

# â­ 6. Dataset Description

Each dataset includes:
| Column | Meaning |
|--------|---------|
| `text` | Journal entry |
| `label` | Numeric confidence level |
| `label_name` | Text version of label |

Datasets are kept in their respective week folders.

---

# â­ 7. Reproducibility Notes

To re-run the entire project:

1. Clone/download repo  
2. Activate virtual environment  
3. Install dependencies  
4. Run notebooks *in chronological order*  
5. Ensure CSV inputs remain in expected folders  
6. For GPU acceleration (optional):  
```
pip install accelerate
```

---

# â­ 8. Loading the Final Model Programmatically
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_path = "week9-10_dataset/week9-10_retrained_distilbert"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
```

---

# â­ 9. License
This project contains personal journal data.  
**Do not redistribute without permission.**

---

# â­ 10. Contact
**Earnest Kyle**  
earnest.kyle@ucdenver.edu
